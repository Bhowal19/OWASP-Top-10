# Labs and Exploitation Flows â€“ A03 Injection

This section is designed to turn understanding into skill.

Injection is best learned by seeing how small trust failures escalate into total compromise.

ðŸ§ª Lab 1 â€“ Classic SQL Injection Flow
An application accepts a username and password.
The backend constructs a SQL query dynamically.

The attacker begins with simple input to observe behavior.
Errors reveal database interaction.
Logical operators are introduced.
Authentication logic collapses.

The attacker confirms injection.
They enumerate tables.
They extract data.

The lesson is not the payload.
It is how **assumptions fail silently**.

ðŸ§ª Lab 2 â€“ API Filter Injection
An API endpoint allows users to filter results using query parameters.

The backend converts these parameters directly into a query object.
The attacker manipulates the filter syntax.
Unexpected conditions are evaluated.
Unauthorized data is returned.

This lab demonstrates that structured input does not equal safe input.

ðŸ§ª Lab 3 â€“ GraphQL Resolver Abuse
A GraphQL API exposes nested relationships.
Resolvers trust incoming arguments.

The attacker crafts deeply nested queries.
They manipulate resolver logic.
They extract sensitive relationships not intended for exposure.

This lab highlights injection at the logic layer, not the syntax layer.

ðŸ§ª Lab 4 â€“ AI Prompt Injection
An AI assistant is configured with system instructions and user input in a single context.

The attacker embeds instructions inside a harmless-looking request.
The model follows attacker intent.
Internal data is exposed.

This lab proves that AI systems are interpretersâ€”and interpreters are injectable.

ðŸ§  Unified Takeaway â€“ Injection Everywhere
Injection is not a vulnerability tied to SQL, APIs, or AI.
It is a design failure that appears whenever:

- Instructions and data are mixed
- Trust boundaries are unclear
- Interpreters act on unvalidated input

Modern systems have more interpreters than ever.
Databases.
Shells.
APIs.
Workflow engines.
LLMs.

Injection survives because systems keep repeating the same mistake in new forms.

The solution has never changed:
Separate intent from input.
Enforce boundaries.
Assume abuse.